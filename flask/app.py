from flask import Flask, request, jsonify
import requests
from bs4 import BeautifulSoup

from flask_cors import CORS
from transformers import pipeline, TextStreamer
import re
from email_utils import send_email
from db_utils import save_template
import threading
import os
from dotenv import load_dotenv
import torch
import time

from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from huggingface_hub import hf_hub_download

load_dotenv()


# Check if CUDA is available
USE_CUDA = torch.cuda.is_available()
device = "cuda" if USE_CUDA else "cpu"
print(f"CUDA Available: {USE_CUDA}, Using Device: {device}")


#testing
TEST_MODE = os.environ.get("TEST_MODE") == "1"


app = Flask(__name__)

# Enable CORS for all routes
CORS(app)

# Credentials
SENDER_EMAIL = "crisiseventtemplant@gmail.com"
SENDER_PASSWORD = "awhh hmvo syfp yiyn"  # Use env vars in prod!

# --- 1) SETUP THE MODEL IN A GLOBAL SCOPE ---
# It is generally better to load large models once on application start.
# This can help avoid re-initializing the model on every request.
# One issue is that your model is large (16GB). Ensure your environment can handle it.
# try:
#     text_generation_pipeline = pipeline("text-generation", 
#         model="TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF",  # Use a long-context model
#         # max_length=2048,  # Increase max tokens
#         truncation=True,  # Ensure no overflow issues)
#         device=0 if USE_CUDA else -1  # Use GPU if available, otherwise CPU
#     )
# except Exception as e:
#     text_generation_pipeline = None
#     print(f"Failed to load model pipeline. Error: {e}")
# Load LlamaCpp model - Adjust with your Llama model path
model_path = hf_hub_download(repo_id="TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF", filename="capybarahermes-2.5-mistral-7b.Q4_K_M.gguf", cache_dir=".")

# The number of layers of the model that are offloaded to your GPU (Graphics Processing Unit).
# In a transformer (LLM) model, the model architecture is often composed of multiple layers. Each layer performs a specific transformation on the input data.
# Offloading layers to the GPU can significantly accelerate computations, as GPUs are optimized for parallel processing.
n_gpu_layers = -1  # Metal set to 1 is enough. If -1, all layers are offloaded.

# how many tokens are processed in parallel for prediction, default is 8. You can set to a bigger number
n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip / CUDA GPU.

# The context size is the maximum number of tokens that the model can account for when processing a response. this includes the prompt, and the response itself, so the context needs to be set large enough for both the question, and answer. The important thing to note, is that the model must process its own response as part of the context in order to write it, since it can only predict one token at a time
# Most models are trained with a context size of 2048. Going over a models context limit is advised against, since it hasn't been trained to account for data sets larger than its suggested context limit.
# If n_ctx is set higher than 512, the model will process the input in chunks of 512 tokens at a time.
# This means it will generate text based on the most recent 512 tokens of context, potentially leading to inconsistencies if the context spans beyond that window.
n_ctx = 7000

#   if top_k > 0: keep only top k tokens with highest probability (top-k filtering). (I didn't use here)
top_k = None

#   if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).
top_p = 0.8

# Number of tokens to generate
# Default: Unlimited, depending on n_ctx.
max_tokens = 4000

# Explained in the slides
temperature = 0.2

llm = LlamaCpp(
      model_path=model_path,
      n_gpu_layers=n_gpu_layers,
      n_batch=n_batch,
      f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls if Metal.
    #   callbacks=callback_manager,
      n_ctx=n_ctx,
      verbose=False,
      max_tokens=max_tokens,
      temperature=temperature, # Critical for good results
      top_p=top_p,
  )

# --- 2) HELPER FUNCTIONS (SCRAPE, GENERATE SUMMARY, PARSE) ---
#the backgorund generation - now the endpoints just generate a autogenerated response 
# while the actual tempante generation happens in this method 
def background_generate_and_notify(user, category, source_text):
    try:
        if TEST_MODE:
            print("Running in TEST MODE")
            template = f"<dummy-tag> Template for {category} generated from test input."
            attributes = ["dummy-tag"]
        else:
            start_time = time.time()  # Start timer
            template = generate_template(category, source_text)
            end_time = time.time()  # End timer
            attributes = parse_attributes(template)

        # Save to MongoDB
        result = save_template(user['email'], category, template, attributes)

        # Verify insert
        if result and result.inserted_id:
            print(f"[MongoDB] Inserted Template for {user['email']} with ID: {result.inserted_id}")
        else:
            print(f"[MongoDB] Failed to insert Template for {user['email']}")

        # Send email
        body = (
            f"Hi {user['name']},\n\n"
            f"Your Template for '{category}' has been generated.\n\n"
            f"Template:\n{template}\n\n"
            f"Attributes: {', '.join(attributes)}\n\n"
            f"Template generation took {end_time - start_time:.2f} seconds\n\n"
            "You can now return to the site to view the summary."
        )
        send_email(SENDER_EMAIL, SENDER_PASSWORD, user['email'],
                   f"Your {category} Template is Ready!", body)

    except Exception as e:
        print(f"Error in background processing: {e}")

def scrape_body_text(url):
    """
    Given a URL, scrape all text content contained in <p> tags.
    Return the combined text as a string.
    """
    response = requests.get(url)
    if response.status_code != 200:
        raise ValueError(f"Received non-200 status code while scraping: {response.status_code}")
    soup = BeautifulSoup(response.content, 'html.parser')
    paragraphs = soup.find_all('p')
    body_text = "\n".join([p.get_text() for p in paragraphs])
    return body_text


def generate_template(disaster_type: str, disaster_context: str) -> str:
    template = """You are a professional news reporter. Your task is to generate a **generalized summary template** for {disaster_type}, ensuring that it applies to any event of this type.

Each input paragraph describes a past {disaster_type} event. Identify **common key attributes** across all examples and structure them into a **standardized template**.

### **Instructions:**
The template **must be fully generalized**â€”DO NOT insert specific details.  
Use **descriptive placeholder tags** in this format: <attribute-name>.  
**DO NOT** add extra commentary or change the output structure.  
The template **must** remain neutral and applicable to all {disaster_type} events.
The template must end with a <unique-extra-info> tag for unique info about a specific {disaster_type}.

### **Output Format:**
Hurricane <hurricane-name> was a Category <category> hurricane that affected <primary-affected-location> and caused <primary-effect> on <hurricane-date>. The hurricane had winds up to <max-wind-speed>. Hurricane <hurricane-name> also affected <secondary-affected-area>. <unique-extra-info>.

Examples of {disaster_type}s: {disaster_context}

**Return ONLY the structured template above without explanations or additional text.**
"""

    prompt = PromptTemplate(input_variables=["disaster_type", "disaster_context"], template=template)
    chain = LLMChain(llm=llm, prompt=prompt)
    
    # Run the prompt through LangChain
    result = chain.run(disaster_type=disaster_type, disaster_context=disaster_context)
    
    return result.strip()


def parse_attributes(template: str) -> list[str]:
    attributes = re.findall(r"<(.*?)>", template)

    return attributes

# --- 3) FLASK ENDPOINTS ---

@app.route('/', methods=['GET'])
def index():
    """
    A simple health-check endpoint.
    """
    return jsonify({"message": "Server is up and running"}), 200

@app.route('/generate_from_url', methods=['POST'])
def generate_from_url():
    """
    This endpoint accepts a URL, disaster category, and email.
    It:
    1. Scrapes the text from the URL.
    2. Immediately returns a response to the user saying the Template is being generated.
    3. In the background:
        - Generates the Template using the model
        - Extracts attributes
        - Saves it to MongoDB
        - Emails the user
    """
    data = request.get_json()

    if not data or 'url' not in data or 'category' not in data or 'email' not in data:
        return jsonify({"error": "Request JSON must contain 'url', 'category', and 'email'."}), 400

    url = data['url']
    category = data['category']
    recipient_email = data['email']

    try:
        # Scrape the article content
        scraped_content = scrape_body_text(url)

        # Launch generation and email in background
        thread = threading.Thread(
            target=background_generate_and_notify,
            args=(recipient_email, category, scraped_content)
        )
        thread.start()

        # Immediate response to user
        return jsonify({
            "message": "Your Template is being generated. You will receive an email when it's ready."
        }), 202

    except Exception as e:
        return jsonify({"error": f"Failed to process request: {e}"}), 500


@app.route('/generate_from_text', methods=['POST'])
def generate_from_text():
    """
    This endpoint accepts raw disaster text, disaster category, and user object.
    It:
    1. Immediately returns a response to the user saying the Template is being generated.
    2. In the background:
        - Generates the Template using the model
        - Extracts attributes
        - Saves it to MongoDB
        - Emails the user
    """
    data = request.get_json()

    if not data or 'text' not in data or 'category' not in data or 'user' not in data:
        return jsonify({"error": "Request JSON must contain 'text', 'category', and 'user' object."}), 400

    raw_text = data['text']
    category = data['category']
    user = data['user']

    if not all(key in user for key in ['name', 'email', 'id', 'role']):
        return jsonify({"error": "User object must contain 'name', 'email', '_id', and 'role'."}), 400

    try:
        # Launch generation and email in background
        thread = threading.Thread(
            target=background_generate_and_notify,
            args=(user, category, raw_text)
        )
        thread.start()

        # Immediate response to user
        return jsonify({
            "message": "Your Template is being generated. You will receive an email when it's ready."
        }), 202

    except Exception as e:
        return jsonify({"error": f"Failed to process request: {e}"}), 500



# --- 4) RUN THE SERVER ---
if __name__ == '__main__':
    # debug=True reloads code automatically, only use for development
    app.run(host='0.0.0.0', port=5000, debug=True)
